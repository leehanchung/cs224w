{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XCS224W_Colab5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leehanchung/cs224w/blob/main/notebooks/XCS224W_Colab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuXWJLEm2UWS"
      },
      "source": [
        "# **CS224W - Colab 5**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gzsP50bF6Gb"
      },
      "source": [
        "In this final Colab we will experiment with advanced topics in GNNs. Specifically, we will look at different techniques for scaling up GNNs using PyTorch Geometric, DeepSNAP and NetworkX. \n",
        "\n",
        "First, we will work with PyTorch Geometric's `NeighborSampler` to scale up training and testing on the OGB `arxiv` dataset.\n",
        "\n",
        "Then, using DeepSNAP and NetworkX, we will implement our own simplified version of `NeighborSampler` and run experiments with different sampling ratios on the Cora graph.\n",
        "\n",
        "Lastly, we will partition the Cora graph into clusters by using different partition algorithms and then train the models using a vanilla Cluster-GCN.\n",
        "\n",
        "**Note**: Make sure to **sequentially run all the cells in each section**, so that the intermediate variables / packages will carry over to the next cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSaetj53YnT6"
      },
      "source": [
        "# Device\n",
        "You likely will want to us a GPU for this Colab.\n",
        "\n",
        "Please click `Runtime` and then `Change runtime type`. Then set the `hardware accelerator` to **GPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67gOQITlCNQi"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_m9l6OYCQZP"
      },
      "source": [
        "# Install torch geometric\n",
        "import os\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu111.html\n",
        "  !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu111.html\n",
        "  !pip install torch-geometric\n",
        "  !pip install -q ogb\n",
        "  !pip install -q git+https://github.com/snap-stanford/deepsnap.git\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRfgbfTjCRD_"
      },
      "source": [
        "import torch_geometric\n",
        "torch_geometric.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxkYLgxAOxz7"
      },
      "source": [
        "# 1) Neighbor Sampling\n",
        "\n",
        "Neighbor Sampling, originally proposed in **GraphSAGE** ([Hamilton et al. (2017)](https://arxiv.org/abs/1706.02216)), is a representative method to scale up GNNs. As we learned in lecture, rather than loading the entire graph into memory for each training loop, we can instead sample a mini-batch of the nodes we want to embed and **only** load the K-hop graph neighborhoods needed to embed these nodes. In this way we take advantage of the fact that the embedding of a node u only depends on its K-hop neighborhood. To further reduce the memory footprint and computational cost, we can choose to sample only a subset of a node's neighborhood during message passing and aggregation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kho6SHUVO1ny"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1WJLGKsOx_k"
      },
      "source": [
        "import copy\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.loader import NeighborSampler\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKqZWqRbO7km"
      },
      "source": [
        "## PyTorch Geometric Neighbor Sampler\n",
        "\n",
        "PyTorch Geometric has implemented Neighbor Samplinging through the [NeighborSampler](https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html#torch_geometric.loader.NeighborSampler) class. \n",
        "Neighbor sampling is based on building a node’s computation graph without storing irrelevant information for a given node, thus, making it more efficient. Each node produces a single computation graph, where for each node in a k-hop neighborhood, at most, $H_k$ neighbors are randomly sampled. Each node's  computation graph will therefore involve $\\prod^K_{k=1} H_k$ leaf nodes for a K-layer GNN. The successive layers of each node's computation graph can be conceptualized as bi-partite graphs that represent each layer of message passing in the GNN as shown in figure below. The blue (or black) dots are source nodes residing at layer $k-1$, while the red dots represent target nodes at subsequent layer $k$.  It is important to stress the target nodes that are embedd in each layer are the final nodes in the left hand side of the bi-partite graph. Node computation graphs are combined when subsequently forming a batch. If you'd like to learn more about information on neighborhood sampling, this\n",
        "**[blog](https://towardsdatascience.com/sampling-large-graphs-in-pytorch-geometric-97a6119c41f9)** provides an excellent description.\n",
        "\n",
        "![img]( https://drive.google.com/uc?export=view&id=1QqcrEsN-HpSHgwHiOD4Dh6yIawZh0Pgj)\n",
        "\n",
        "\n",
        "**PyG Docs**\n",
        "\n",
        "The neighbor sampler from the “Inductive Representation Learning on Large Graphs” paper, which allows for mini-batch training of GNNs on large-scale graphs where full-batch training is not feasible.\n",
        "\n",
        "Given a GNN with  layers and a specific mini-batch of nodes node_idx for which we want to compute embeddings, this module iteratively samples neighbors and constructs bipartite graphs that simulate the actual computation flow of GNNs.\n",
        "\n",
        "More specifically, sizes denotes how much neighbors we want to sample for each node in each layer. This module then takes in these sizes and iteratively samples sizes[l] for each node involved in layer l. In the next layer, sampling is repeated for the union of nodes that were already encountered. The actual computation graphs are then returned in reverse-mode, meaning that we pass messages from a larger set of nodes to a smaller one, until we reach the nodes for which we originally wanted to compute embeddings.\n",
        "\n",
        "Hence, an item returned by NeighborSampler holds the current batch_size, the IDs n_id of all nodes involved in the computation, and a list of bipartite graph objects via the tuple (edge_index, e_id, size), where edge_index represents the bipartite edges between source and target nodes, e_id denotes the IDs of original edges in the full graph, and size holds the shape of the bipartite graph. For each bipartite graph, target nodes are also included at the beginning of the list of source nodes so that one can easily apply skip-connections or add self-loops.\n",
        "\n",
        "\n",
        "If you are interested in memory-efficient aggregations, please refer to PyG's [Memory-Efficient Aggregations](https://pytorch-geometric.readthedocs.io/en/latest/notes/sparse_tensor.html).  Following is an example that uses the Neighbor Sampling method on training the OGB `arxiv` dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWlyStlRO6_u"
      },
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  dataset_name = 'ogbn-arxiv'\n",
        "  dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                  transform=T.ToSparseTensor())\n",
        "  data = dataset[0]\n",
        "  data.adj_t = data.adj_t.to_symmetric()\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  print('Device: {}'.format(device))\n",
        "\n",
        "  data = data.to(device)\n",
        "  split_idx = dataset.get_idx_split()\n",
        "  train_idx = split_idx['train'].to(device)\n",
        "\n",
        "  sampled_subgraph_batch_loader = None\n",
        "  full_subgraph_loader = None\n",
        "  \n",
        "  ############# Your code here ############\n",
        "  ## (~2 line of code)\n",
        "  ## Note:\n",
        "  ## 1. Construct the NeighborSampler `sampled_subgraph_batch_loader`. \n",
        "  ##    Use a batch size of 4096, turn shuffle on, and only \n",
        "  ##    use train_idx nodes to create mini-batches. During sampling,\n",
        "  ##    sample up to 10 neighbors in layer one and 5 neighbors in layer 2.\n",
        "  ## 2. Construct the NeighborSampler `full_subgraph_loader`. \n",
        "  ##    Use a batch size of 4096 and turn shuffle off. Sample all neighbors\n",
        "  ##    for only ONE layer and consider all nodes for sampling mini-batches!\n",
        "  ##    We use this loader for the inference / test phase of our model. In the\n",
        "  ##    inference function we will why we only need to sample complete\n",
        "  ##    1-hop neighborhoods.\n",
        "  ## 3. Look at the NeighborSampler documentation to figure out which \n",
        "  ##    parameters you need to set:\n",
        "  ##    https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html#torch_geometric.loader.NeighborSampler\n",
        "\n",
        "  \n",
        "  #################################################################################\n",
        "\n",
        "  evaluator = Evaluator(name='ogbn-arxiv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjdkIcFpRYyl"
      },
      "source": [
        "## GNN Model\n",
        "\n",
        "After creating our `NeighborSampler`, we also need to modify our model to support the mini-batch training.\n",
        "\n",
        "The `forward` function will take the node feature `x` and a list of three-element tuples `adjs`. Each element in `adjs` contains following elements:\n",
        "* `edge_index`: The edge index tensor between source and destination nodes, which forms a bipartite grpah.\n",
        "* `e_id`: The indices of the edges in the original graph.\n",
        "* `size`: The shape of the bipartite graph, in (*number of source nodes*, *number of destination nodes*) format.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRBJS_5qRWbu"
      },
      "source": [
        "class SAGE(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
        "                 dropout):\n",
        "        super(SAGE, self).__init__()\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "\n",
        "        self.convs.append(SAGEConv(input_dim, hidden_dim))\n",
        "        self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "        for i in range(num_layers - 2):\n",
        "            self.convs.append(\n",
        "                SAGEConv(hidden_dim, hidden_dim))\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
        "        self.convs.append(SAGEConv(hidden_dim, output_dim))\n",
        "\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adjs, mode=\"batch\"):\n",
        "        if mode == \"batch\":\n",
        "            ############# Your code here ############\n",
        "            ## (~7 line of code)\n",
        "            ## Note:\n",
        "            ## 1. Loop through the list `adjs` and apply L GNN layers.\n",
        "            ##    Refer to the description above for the elements in each tuple\n",
        "            ##    adjs[l].\n",
        "            ## 2. Our GNN model is of the form:\n",
        "            ##      conv -> bn -> relu -> dropout -> ... -> conv\n",
        "            ## 3. As described above, each layer is defined by a bipartite graph\n",
        "            ##    between (source nodes and target nodes), where the size parameter\n",
        "            ##    tells us many source nodes and target nodes we have.\n",
        "            ## 4. Rather than passing just x to the SAGEConv layer, you can pass\n",
        "            ##    a tuple of the form (x_src, x_target). With this formulation\n",
        "            ##    we only produce embeddings for the `x_target` nodes and use\n",
        "            ##    `x_source` as the nodes needed for message passing.  \n",
        "            ##\n",
        "            ##    Hint: \n",
        "            ##      - Target nodes are included as the first nodes in the source nodes.\n",
        "            ##      - The target nodes for layer (l) become the source nodes \n",
        "            ##      for layer (l+1)!\n",
        "\n",
        "            pass  \n",
        "\n",
        "            #####################################\n",
        "        else:\n",
        "            for i, conv in enumerate(self.convs):\n",
        "                x = conv(x, adjs)\n",
        "                if i != self.num_layers - 1:\n",
        "                    x = self.bns[i](x)\n",
        "                    x = F.relu(x)\n",
        "                    x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.softmax(x)\n",
        "    \n",
        "    def inference(self, x_all, all_loader):\n",
        "        # This function will be called in test\n",
        "        for i in range(self.num_layers):\n",
        "            xs = []\n",
        "            ############# Your code here ############\n",
        "            ## (~7 line of code)\n",
        "            ## Note:\n",
        "            ## 1. Very similar idea to the forward function!\n",
        "            ## 2. Looping through all_loader to apply conv layer i to each\n",
        "            ##    batch of nodes, where all_loader returns tuples of:\n",
        "            ##      - batch_size\n",
        "            ##      - node_indeces of the nodes we want to sample from x_all\n",
        "            ##      in the current batch.\n",
        "            ##      - adj: see above for what is in the tuple adj.\n",
        "            ## 3. After selecting nodes using the node_indeces, follow the same\n",
        "            ##    steps as the forward function.\n",
        "            ## 4. Since we are doing mini-batches of nodes, we now need\n",
        "            ##    to append the generated embeddings to compute all\n",
        "            ##    the node embeddings!\n",
        "            ## 5. Rember to move `adj` and the selected nodes for each batch to\n",
        "            ##    the GPU `device`.\n",
        "\n",
        "            pass\n",
        "\n",
        "            #####################################\n",
        "            \n",
        "            # Concatenate all updated embeddings into one tensor.\n",
        "            # We simulate the update process at the end of message\n",
        "            # passing. Because of this we only have to sample 1-hop neighborhoods\n",
        "            # for our full_subgraph_loader!\n",
        "            x_all = torch.cat(xs, dim=0)\n",
        "\n",
        "        return x_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cfm7K3wRqqY"
      },
      "source": [
        "## Training and Testing\n",
        "\n",
        "Now lets implement the training and testing functions.\n",
        "\n",
        "In both training and testing, we need to sample batch from the dataloader.\n",
        "\n",
        "Each batch in the `NeighborSampler` dataloader holds three elements:\n",
        "* `batch_size`: The batch size specified in the dataloader.\n",
        "* `n_id`: All nodes (in index format) used in the adjacency matrices.\n",
        "* `adjs`: The three-element tuples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JN0-_QCRn8N"
      },
      "source": [
        "def train(model, data, train_loader, train_idx, optimizer, loss_fn, mode=\"batch\"):\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    if mode == \"batch\":\n",
        "        for batch_size, n_id, adjs in train_loader:\n",
        "            # Move all adj sparse tensors to GPU\n",
        "            adjs = [adj.to(device) for adj in adjs]\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Index on the node features\n",
        "            out = model(data.x[n_id], adjs)\n",
        "            train_label = data.y[n_id[:batch_size]].squeeze(-1)\n",
        "            loss = loss_fn(out, train_label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "    else:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.adj_t, mode=mode)[train_idx]\n",
        "        train_label = data.y.squeeze(1)[train_idx]\n",
        "        loss = loss_fn(out, train_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss = loss.item()\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, data, all_loader, split_idx, evaluator, mode=\"batch\", save_model_results=False):\n",
        "    model.eval()\n",
        "\n",
        "    if mode == \"batch\":\n",
        "        out = model.inference(data.x, all_loader)\n",
        "    else:\n",
        "        out = model(data.x, data.adj_t, mode=\"all\")\n",
        "\n",
        "    y_true = data.y.cpu()\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': y_true[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': y_true[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': y_true[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    if save_model_results:\n",
        "      print (\"Saving Model Predictions\")\n",
        "\n",
        "      data = {}\n",
        "      data['y_pred'] = y_pred.view(-1).cpu().detach().numpy()\n",
        "\n",
        "      df = pd.DataFrame(data=data)\n",
        "      # Save locally as csv\n",
        "      df.to_csv('ogbn-arxiv_' + mode + '.csv', sep=',', index=False)\n",
        "\n",
        "    return train_acc, valid_acc, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiehZ8OiR2q9"
      },
      "source": [
        "## Mini-batch Training\n",
        "\n",
        "Test our model using mini-batch training, based on our NeighborSampler!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFaI2eCARy0v"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  args = {\n",
        "      'device': device,\n",
        "      'num_layers': 2,\n",
        "      'hidden_dim': 128,\n",
        "      'dropout': 0.5,\n",
        "      'lr': 0.01,\n",
        "      'epochs': 100,\n",
        "  }\n",
        "\n",
        "  batch_model = SAGE(data.num_features, args['hidden_dim'],\n",
        "              dataset.num_classes, args['num_layers'],\n",
        "              args['dropout']).to(device)\n",
        "  batch_model.reset_parameters()\n",
        "\n",
        "  optimizer = torch.optim.Adam(batch_model.parameters(), lr=args['lr'])\n",
        "  loss_fn = F.nll_loss\n",
        "\n",
        "  best_batch_model = None\n",
        "  best_valid_acc = 0\n",
        "\n",
        "  batch_results = []\n",
        "\n",
        "  for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "      loss = train(batch_model, data, sampled_subgraph_batch_loader, train_idx, optimizer, loss_fn, mode=\"batch\")\n",
        "      result = test(batch_model, data, full_subgraph_loader, split_idx, evaluator, mode=\"batch\")\n",
        "      batch_results.append(result)\n",
        "      train_acc, valid_acc, test_acc = result\n",
        "      if valid_acc > best_valid_acc:\n",
        "          best_valid_acc = valid_acc\n",
        "          best_batch_model = copy.deepcopy(batch_model)\n",
        "      print(f'Epoch: {epoch:02d}, '\n",
        "            f'Loss: {loss:.4f}, '\n",
        "            f'Train: {100 * train_acc:.2f}%, '\n",
        "            f'Valid: {100 * valid_acc:.2f}% '\n",
        "            f'Test: {100 * test_acc:.2f}%')\n",
        "  best_result = test(best_batch_model, data, full_subgraph_loader, split_idx, evaluator, mode=\"batch\", save_model_results=True)\n",
        "  train_acc, valid_acc, test_acc = best_result\n",
        "  print(f'Best model: '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwcRKcAh16RV"
      },
      "source": [
        "## **Question 1:** What is the maximum accuracy obtained on the test set using mini-batch training? (10 points)\n",
        "\n",
        "Running the cell above will show the results of your best model and save your best model's predictions to a file named ogbn-arxiv_batch.csv'.\n",
        "\n",
        "As we have seen before you can view this file by clicking on the Folder icon on the left side pannel. When you sumbit your assignment, you will have to download this file and attatch it to your submission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OyqW-1pSMLW"
      },
      "source": [
        "## Full-batch Training\n",
        "\n",
        "Now for reference, we will compare training over all the nodes using full-batch mode (i.e. as we have done in the previous Colabs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mU5eAviTSFMO"
      },
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:  \n",
        "  # Use the same parameters for a full-batch training\n",
        "  args = {\n",
        "      'device': device,\n",
        "      'num_layers': 2,\n",
        "      'hidden_dim': 128,\n",
        "      'dropout': 0.5,\n",
        "      'lr': 0.01,\n",
        "      'epochs': 100,\n",
        "  }\n",
        "\n",
        "  all_model = SAGE(data.num_features, args['hidden_dim'],\n",
        "              dataset.num_classes, args['num_layers'],\n",
        "              args['dropout']).to(device)\n",
        "  all_model.reset_parameters()\n",
        "\n",
        "  optimizer = torch.optim.Adam(all_model.parameters(), lr=args['lr'])\n",
        "  loss_fn = F.nll_loss\n",
        "\n",
        "  best_all_model = None\n",
        "  best_valid_acc = 0\n",
        "\n",
        "  all_results = []\n",
        "\n",
        "  for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "      # NOTE: For the full batch model, the NeighborSampler loader is not used!\n",
        "      loss = train(all_model, data, sampled_subgraph_batch_loader, train_idx, optimizer, loss_fn, mode=\"all\")\n",
        "      result = test(all_model, data, full_subgraph_loader, split_idx, evaluator, mode=\"all\")\n",
        "      all_results.append(result)\n",
        "      train_acc, valid_acc, test_acc = result\n",
        "      if valid_acc > best_valid_acc:\n",
        "          best_valid_acc = valid_acc\n",
        "          best_all_model = copy.deepcopy(all_model)\n",
        "      print(f'Epoch: {epoch:02d}, '\n",
        "            f'Loss: {loss:.4f}, '\n",
        "            f'Train: {100 * train_acc:.2f}%, '\n",
        "            f'Valid: {100 * valid_acc:.2f}% '\n",
        "            f'Test: {100 * test_acc:.2f}%')\n",
        "  best_result = test(best_all_model, data, full_subgraph_loader, split_idx, evaluator, mode=\"all\")\n",
        "  train_acc, valid_acc, test_acc = best_result\n",
        "  print(f'Best model: '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrECcOQQSZo1"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh_qvSG1SV63"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  batch_results = np.array(batch_results)\n",
        "  all_results = np.array(all_results)\n",
        "\n",
        "  x = np.arange(1, 101)\n",
        "\n",
        "  plt.figure(figsize=(9, 7))\n",
        "\n",
        "  plt.plot(x, batch_results[:, 1], label=\"Batch Validation\")\n",
        "  plt.plot(x, batch_results[:, 2], label=\"Batch Test\")\n",
        "  plt.plot(x, all_results[:, 1], label=\"All Validation\")\n",
        "  plt.plot(x, all_results[:, 2], label=\"All Test\")\n",
        "  plt.title('Model Accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFb2OAvOSn_O"
      },
      "source": [
        "# 2) Neighbor Sampling with Different Ratios\n",
        "\n",
        "Next, we will implement our own simplified version of Neighbor Sampling using DeepSNAP and NetworkX. Then we will use our sampler to train models with different neighborhood sampling ratios and compare their performance.\n",
        "\n",
        "To make our experiments faster, we will use the Cora graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9U0F7bnSz9u"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUF4on-fSxcq"
      },
      "source": [
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch.nn import Sequential, Linear, ReLU\n",
        "from deepsnap.dataset import GraphDataset\n",
        "from deepsnap.graph import Graph\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  pyg_dataset = Planetoid('./tmp', \"Cora\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw6k-KdFTEYw"
      },
      "source": [
        "## GNN Model\n",
        "\n",
        "We use a simple GraphSage GNN model, which we provide to you below. Similar to in section one, notice the slightly different implementations of the forward method depending on the data `mode`. When `mode = \"batch\"` we use Neighbor sampling. Thus, the data parameter contains our graphs node features (`x`) and a list `edge_indeces` containing the connectivity of each GNN layer (i.e. an edge_index for each layer, defining the bipartite neighborhood computation graph). \n",
        "\n",
        "**NOTE:** Refer to sections *Neighbor Sampling* and *PyTorch Geometric Neighbor Sampler* above for a detailed overview of the Neighbor Sampling technique."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvUlNi2TS09i"
      },
      "source": [
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, args):\n",
        "        super(GNN, self).__init__()\n",
        "        self.dropout = args['dropout']\n",
        "        self.num_layers = args['num_layers']\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.bns = nn.ModuleList()\n",
        "\n",
        "        self.convs.append(SAGEConv(input_dim, hidden_dim))\n",
        "        self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "        for l in range(self.num_layers - 2):\n",
        "            self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
        "            self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
        "        self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
        "\n",
        "        self.post_mp = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data, mode=\"batch\"):\n",
        "        # Observe the difference between mode == \"batch\" and mode == \"all\".\n",
        "        # In mode == \"batch\" we pass in an edge index for each conv layer\n",
        "        # corresponding to that layer's bipartite graph structure.\n",
        "        if mode == \"batch\":\n",
        "            edge_indices, x = data\n",
        "            for i in range(len(self.convs) - 1):\n",
        "                edge_index = edge_indices[i]\n",
        "                x = self.convs[i](x, edge_index)\n",
        "                x = self.bns[i](x)\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            x = self.convs[-1](x, edge_indices[len(self.convs) - 1])\n",
        "        else:\n",
        "            x, edge_index = data.node_feature, data.edge_index\n",
        "            for i in range(len(self.convs) - 1):\n",
        "                x = self.convs[i](x, edge_index)\n",
        "                x = self.bns[i](x)\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            x = self.convs[-1](x, edge_index)\n",
        "        x = self.post_mp(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulp1A3evcJ-I"
      },
      "source": [
        "## Implementing Neighbor Sampling\n",
        "\n",
        "Now let's take a stab at implementing our own basic version of Neighbor Sampling using DeepSNAP and NetworkX. To decompose the process, we will define several helper functions before finaling defining our own `neighbor_sampling` function! \n",
        "\n",
        "**NOTE:** Before working through this section, we highly recommend reviewing sections `Neighbor Sampling` and `PyTorch Geometric Neighbor Sampler`. Specifically, it is important to understand how we explicitly define an `edge_index` for each GNN layer, representing the bipartite computation graph connecting the `target_nodes` that we are embedding for that layer to their relavent neighbors from the previous layer needed for message passing. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWneVr3_Hj4n"
      },
      "source": [
        "## **Question 2.1a**: Implementing the `sample_neighbors` function. (2.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1TZXvJhHjRK"
      },
      "source": [
        "def sample_neighbors(nodes, G, ratio, all_nodes):\n",
        "    # TODO: Implement a function that takes as input a set of nodes, \n",
        "    # a NetworkX graph G, and a neighbor sampling ratio and returns:\n",
        "    #   1. A set of the sampled nodes\n",
        "    #   2. A set union between `all_nodes` and the newly sampled neighbor nodes.\n",
        "    #      This allows us to track the nodes needed across all message passing layers.\n",
        "    #   3. The set of edges connecting the sampled neighboring nodes to our inpute\n",
        "    #      set of nodes. Represents a bi-partite graph between targets (nodes)\n",
        "    #      and source (neighbor) nodes.\n",
        "\n",
        "    neighbors = set()\n",
        "    edges = []\n",
        "\n",
        "    ############# Your code here ############\n",
        "    ## (~8-10 line of code)\n",
        "    ## Note:\n",
        "    ## 1. You will will need to sample neighbors from each node given to you in \n",
        "    ##    `nodes` list. \n",
        "    ##    Hint: Used graph `G` to assist in obtaining the neighbors of each node. \n",
        "    ## 2. Randomly sample neighbors without replacement (i.e. the same neighbors \n",
        "    ##    should not be selected more than once for a given node) \n",
        "    ## 3. The neighbors are stored in a set data structure to ensure that duplicates\n",
        "    ##    are avoided.  This is useful as the set union will be taken with `all_nodes`. \n",
        "    ## 5. The edges list should contain all edges sampled in the form of a tuple\n",
        "    ##    of (neighbor, node)  \n",
        "    \n",
        "    \n",
        "    ########################################## \n",
        "    return neighbors, neighbors.union(all_nodes), edges\n",
        "  \n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  graphs_train, _, _ = \\\n",
        "      GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "  graph_train = graphs_train[0]\n",
        "\n",
        "  nodes = [15, 16, 17]\n",
        "  neighbors_full, _, _ = sample_neighbors(nodes, graph_train.G, 1, set())\n",
        "  neighbors_sampled, _, _ = sample_neighbors(nodes, graph_train.G, 0.3, set())\n",
        "  print (\"Neighbors with ratio = 1:\", neighbors_full)\n",
        "  print (\"Neighbors with ratio = 0.5:\", neighbors_sampled)\n",
        "  # Note that this is not expected to be 0.3. Since we apply\n",
        "  # our sampling ratio for each node, the number of neighbors\n",
        "  # for each node may not evenly divide by the ratio   \n",
        "  print (\"Ratio of sampled neighbors:\", len(neighbors_sampled) / len(neighbors_full))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NKNh4TEJ8_p"
      },
      "source": [
        "## Tensor transformation and node relabeling helper functions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2PeBMJIJ9Tn"
      },
      "source": [
        "def nodes_to_tensor(nodes):\n",
        "    \"\"\"\n",
        "      Transforms a set of nodes into a node index tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    node_label_index = torch.tensor(list(nodes), dtype=torch.long)\n",
        "    return node_label_index\n",
        "\n",
        "\n",
        "def edges_to_tensor(edges):\n",
        "    \"\"\"\n",
        "      Transforms a list of undirected edges into the corresponding PyG\n",
        "      edge_index tensor representation. Notice that we explicitly make\n",
        "      sure to include both edge directions.  \n",
        "    \"\"\"\n",
        "\n",
        "    edge_index = torch.tensor(list(edges), dtype=torch.long)\n",
        "    edge_index = torch.cat([edge_index, torch.flip(edge_index, [1])], dim=0)\n",
        "    edge_index = edge_index.permute(1, 0)\n",
        "\n",
        "    return edge_index\n",
        "\n",
        "def relabel(nodes, labeled_nodes, edges_list):\n",
        "    \"\"\"\n",
        "      Relabel nodes with 0 based indeces.\n",
        "      \n",
        "      During the sampling process, we are likely to sample a list of\n",
        "      non-continuous node ids. However, our GNN models rely on continuous\n",
        "      0 based indexing to index into the rows of our node features matrix \n",
        "      based on edges in the graph (edge_index)\n",
        "    \"\"\"\n",
        "\n",
        "    relabeled_edges_list = []\n",
        "    sorted_nodes = sorted(nodes)\n",
        "    node_mapping = {node : i for i, node in enumerate(sorted_nodes)}\n",
        "    for orig_edges in edges_list:\n",
        "        relabeled_edges = []\n",
        "        for edge in orig_edges:\n",
        "            relabeled_edges.append((node_mapping[edge[0]], node_mapping[edge[1]]))\n",
        "        relabeled_edges_list.append(relabeled_edges)\n",
        "    relabeled_labeled_nodes = [node_mapping[node] for node in labeled_nodes]\n",
        "    relabeled_nodes = [node_mapping[node] for node in nodes]\n",
        "\n",
        "    return relabeled_edges_list, relabeled_nodes, relabeled_labeled_nodes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El-2h_ApOJYP"
      },
      "source": [
        "## **Question 2.1b**: Putting it all together - Implementing our own Neighbor Sampling function. (2.5 points)\n",
        "\n",
        "Now that we've developed a better understanding of what the Neighbor Sampling funciton does, we will implement our own version.  Instead of choosing $H_k$ number of samples at each layer, we will use a ratio of the number of neigbors that a givn node has.  Can you think of the pros and cons of using a ratio of the number of neighbors for a node at different layers?  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI4qHkE4cQOh"
      },
      "source": [
        "def neighbor_sampling(graph, K=2, ratios=(0.1, 0.1, 0.1)):\n",
        "    # TODO: Implement a function that performs Neighbor Sampling on an input\n",
        "    # graph G for a K layer GNN. Notice that len(ratios) = K + 1. Ratios[-1]\n",
        "    # determines size of our mini-batch (i.e. the number of labeled \n",
        "    # nodes we sample computation graphs for). \n",
        "\n",
        "    assert K + 1 == len(ratios)\n",
        "\n",
        "    labeled_nodes = graph.node_label_index.tolist()\n",
        "    random.shuffle(labeled_nodes)\n",
        "    num = int(len(labeled_nodes) * ratios[-1])\n",
        "    if num > 0:\n",
        "        labeled_nodes = labeled_nodes[:num]\n",
        "    nodes_list = [set(labeled_nodes)]\n",
        "    edges_list = []\n",
        "    all_nodes = labeled_nodes\n",
        "\n",
        "    ############# Your code here ############\n",
        "    ## (~4-6 line of code)\n",
        "    ## Note:\n",
        "    ## 1. Using your previously defined `sample_neighbors` function, build up the \n",
        "    ##    nodes_list and edges_list for our K layer network.\n",
        "    ## 2. nodes_list is a list where node_list[i] is set of nodes used for message\n",
        "    ##    passing in layer i+1 of our GNN (i.e. level i in our computation graph).\n",
        "    ##    Notice, node_list[-1] represents the target nodes we want to \n",
        "    ##    embedd in the mini-batch.\n",
        "    ## 3. edge_list is a list of the bi-partite edge conections between layers\n",
        "    ##    in the computation graph.\n",
        "    ## 4. all_nodes is used to track all the nodes needed for message passing.\n",
        "    ## 5. Remember in a GNN, information flows from the base of the computation\n",
        "    ##    graph to the root. How does this affect the way we add to the nodes_list \n",
        "    ##    and edge_list, as well as how we read from ratios (ratios[-1] \n",
        "    ##    represents the root nodes in our computation graph)?\n",
        "\n",
        "\n",
        "    #########################################\n",
        "\n",
        "    relabled_edges_list, relabeled_all_nodes, relabeled_labeled_nodes = \\\n",
        "        relabel(all_nodes, labeled_nodes, edges_list)\n",
        "\n",
        "    node_index = nodes_to_tensor(relabeled_all_nodes)\n",
        "    # All node features that will be used\n",
        "    node_feature = graph.node_feature[node_index]\n",
        "    edge_indices = [edges_to_tensor(edges) for edges in relabled_edges_list]\n",
        "    node_label_index = nodes_to_tensor(relabeled_labeled_nodes)\n",
        "    log = \"Sampled {} nodes, {} edges, {} labeled nodes\"\n",
        "    print(log.format(node_feature.shape[0], edge_indices[0].shape[1] // 2, node_label_index.shape[0]))\n",
        "    return node_feature, edge_indices, node_label_index\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  # Need to define some basic test! Primarily to test whether they build \n",
        "  # in the correct reverse order. So ideally something like ratio = (0.3, 0.5, 0.8).\n",
        "  # Just need to check shapes.\n",
        "  \n",
        "  graphs_train, _, _ = \\\n",
        "      GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "  graph_train = graphs_train[0]\n",
        "\n",
        "  node_feature, edge_indices, node_label_index = neighbor_sampling(graph_train, K=3, ratios=(1, 1, 1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooy6Hcf7TIhI"
      },
      "source": [
        "## Training and Testing\n",
        "\n",
        "Additionally, notice that node classification task on Cora is a semi-supervised classification task, here we keep all the labeled training nodes (140 nodes) by setting the last ratio to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSmZhpzPTGPY"
      },
      "source": [
        "def train(train_graphs, val_graphs, args, model, optimizer, mode=\"batch\"):\n",
        "    best_val = 0\n",
        "    best_model = None\n",
        "    accs = []\n",
        "    graph_train = train_graphs[0]\n",
        "    graph_train.to(args['device'])\n",
        "    for epoch in range(1, 1 + args['epochs']):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        if mode == \"batch\":\n",
        "            node_feature, edge_indices, node_label_index = neighbor_sampling(graph_train, args['num_layers'], args['ratios'])\n",
        "            node_feature = node_feature.to(args['device'])\n",
        "            node_label_index = node_label_index.to(args['device'])\n",
        "            for i in range(len(edge_indices)):\n",
        "                edge_indices[i] = edge_indices[i].to(args['device'])\n",
        "            pred = model([edge_indices, node_feature])\n",
        "            pred = pred[node_label_index]\n",
        "            label = graph_train.node_label[node_label_index]\n",
        "        elif mode == \"community\":\n",
        "            graph = random.choice(train_graphs)\n",
        "            graph = graph.to(args['device'])\n",
        "            pred = model(graph, mode=\"all\")\n",
        "            pred = pred[graph.node_label_index]\n",
        "            label = graph.node_label[graph.node_label_index]\n",
        "        else:\n",
        "            pred = model(graph_train, mode=\"all\")\n",
        "            label = graph_train.node_label\n",
        "            pred = pred[graph_train.node_label_index]\n",
        "        loss = F.nll_loss(pred, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_acc, val_acc, test_acc = test(val_graphs, model)\n",
        "        accs.append((train_acc, val_acc, test_acc))\n",
        "        if val_acc > best_val:\n",
        "            best_val = val_acc\n",
        "            best_model = copy.deepcopy(model)\n",
        "        print(f'Epoch: {epoch:02d}, '\n",
        "              f'Loss: {loss:.4f}, '\n",
        "              f'Train: {100 * train_acc:.2f}%, '\n",
        "              f'Valid: {100 * val_acc:.2f}% '\n",
        "              f'Test: {100 * test_acc:.2f}%')\n",
        "    return best_model, accs\n",
        "\n",
        "def test(graphs, model, save_model_results=False, batch_type=\"batch\", title=None):\n",
        "    model.eval()\n",
        "    accs = []\n",
        "  \n",
        "    for graph in graphs:\n",
        "        graph = graph.to(args['device'])\n",
        "        pred = model(graph, mode=\"all\")\n",
        "        label = graph.node_label\n",
        "        pred = pred[graph.node_label_index].max(1)[1]\n",
        "        acc = pred.eq(label).sum().item()\n",
        "        acc /= len(label)\n",
        "        accs.append(acc)\n",
        "    \n",
        "    if save_model_results:\n",
        "      print (\"Saving Model Predictions for Model:\", batch_type, title)\n",
        "\n",
        "      data = {}\n",
        "      # The last dataset we test is the test graph\n",
        "      data['pred'] = pred.view(-1).cpu().detach().numpy()\n",
        "      data['label'] = label.view(-1).cpu().detach().numpy()\n",
        "\n",
        "      df = pd.DataFrame(data=data)\n",
        "      # Save locally as csv\n",
        "      file_name = 'CORA_Node_' + batch_type\n",
        "      if title is not None:\n",
        "        file_name = file_name + \"_\" + title\n",
        "\n",
        "      df.to_csv(file_name + '.csv', sep=',', index=False)\n",
        "\n",
        "    return accs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV7i0v0ETKzf"
      },
      "source": [
        "args = {\n",
        "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "    'dropout': 0.5,\n",
        "    'num_layers': 2,\n",
        "    'hidden_size': 64,\n",
        "    'lr': 0.005,\n",
        "    'epochs': 50,\n",
        "    'ratios': (0.8, 0.8, 1),\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLpRYKbnTQnj"
      },
      "source": [
        "## Full-Batch Training\n",
        "\n",
        "As a baseline, we train our GNN model over the entire graph without any Neighbor Sampling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMGGjbJBTOo1"
      },
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  graphs_train, graphs_val, graphs_test = \\\n",
        "      GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "  graph_train = graphs_train[0]\n",
        "  graph_val = graphs_val[0]\n",
        "  graph_test = graphs_test[0]\n",
        "\n",
        "  model = GNN(graph_train.num_node_features, args['hidden_size'], graph_train.num_node_labels, args).to(args['device'])\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "  graphs = [graph_train, graph_val, graph_test]\n",
        "  all_best_model, all_accs = train(graphs, graphs, args, model, optimizer, mode=\"all\")\n",
        "  train_acc, val_acc, test_acc = test([graph_train, graph_val, graph_test], all_best_model)\n",
        "  print('Best model:',\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * val_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWkGiwB6Thr4"
      },
      "source": [
        "## **Question 2.2a:** What is the maximum test accuracy using samping ratios = (0.7, 0.9, 1)? (7.5 points)\n",
        "\n",
        "Running the cell below will show the results of your best model and save your best model's predictions to a file named CORA_Node_batch_(0.7, 0.9, 1).csv'.\n",
        "\n",
        "As we have seen before you can view this file by clicking on the Folder icon on the left side pannel. When you sumbit your assignment, you will have to download this file and attatch it to your submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWusJ9u3Tfhv"
      },
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  args['ratios'] = (0.7, 0.9, 1)\n",
        "\n",
        "  graphs_train, graphs_val, graphs_test = \\\n",
        "      GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "  graph_train = graphs_train[0]\n",
        "  graph_val = graphs_val[0]\n",
        "  graph_test = graphs_test[0]\n",
        "\n",
        "  model = GNN(graph_train.num_node_features, args['hidden_size'], graph_train.num_node_labels, args).to(args['device'])\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "  graphs = [graph_train, graph_val, graph_test]\n",
        "  batch_best_model, batch_accs = train(graphs, graphs, args, model, optimizer)\n",
        "  train_acc, val_acc, test_acc = test([graph_train, graph_val, graph_test], batch_best_model, save_model_results=True, batch_type=\"batch\", title=\"(0.7,0.9,1)\")\n",
        "  print('Best model:',\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * val_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_FjkNHDT4c6"
      },
      "source": [
        "## **Question 2.2b:** What is the maximum test accuracy using samping ratios = (0.3, 0.5, 1)? (7.5 points)\n",
        "\n",
        "Running the cell below will show the results of your best model and save your best model's predictions to a file named CORA_Node_batch_(0.3, 0.5, 1).csv'.\n",
        "\n",
        "As we have seen before you can view this file by clicking on the Folder icon on the left side pannel. When you sumbit your assignment, you will have to download this file and attatch it to your submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "booJ6DASTjO4"
      },
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  # Change the ratio to 0.3\n",
        "  args['ratios'] = (0.3, 0.5, 1)\n",
        "\n",
        "  graphs_train, graphs_val, graphs_test = \\\n",
        "      GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "  graph_train = graphs_train[0]\n",
        "  graph_val = graphs_val[0]\n",
        "  graph_test = graphs_test[0]\n",
        "\n",
        "  model = GNN(graph_train.num_node_features, args['hidden_size'], graph_train.num_node_labels, args).to(args['device'])\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "  graphs = [graph_train, graph_val, graph_test]\n",
        "  batch_best_model, batch_accs_1 = train(graphs, graphs, args, model, optimizer)\n",
        "  train_acc, val_acc, test_acc = test([graph_train, graph_val, graph_test], batch_best_model, save_model_results=True, batch_type=\"batch\", title=\"(0.3,0.5,1)\")\n",
        "  print('Best model:',\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * val_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EePAvNlGUM2K"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7etNAkXAT55d"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  batch_results = np.array(batch_accs)\n",
        "  batch_results_1 = np.array(batch_accs_1)\n",
        "  all_results = np.array(all_accs)\n",
        "\n",
        "  x = np.arange(1, 51)\n",
        "\n",
        "  plt.figure(figsize=(9, 7))\n",
        "\n",
        "  plt.plot(x, batch_results[:, 0], label=\"Batch 0.8 Train\")\n",
        "  plt.plot(x, batch_results[:, 1], label=\"Batch 0.8 Validation\")\n",
        "  plt.plot(x, batch_results[:, 2], label=\"Batch 0.8 Test\")\n",
        "  plt.plot(x, batch_results_1[:, 0], label=\"Batch 0.3 Train\")\n",
        "  plt.plot(x, batch_results_1[:, 1], label=\"Batch 0.3 Validation\")\n",
        "  plt.plot(x, batch_results_1[:, 2], label=\"Batch 0.3 Test\")\n",
        "  plt.plot(x, all_results[:, 0], label=\"All Train\")\n",
        "  plt.plot(x, all_results[:, 1], label=\"All Validation\")\n",
        "  plt.plot(x, all_results[:, 2], label=\"All Test\")\n",
        "  plt.title('Model Accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkA7-0groq7q"
      },
      "source": [
        "**NOTE:** We always evaluate accuracy in full-batch mode. Namely, only during training do we use neighborhood sub-sampling. We do this for a couple reason: 1) fairness of comparison, 2) we worry most about computational cost during training as compared to evaluation, and 3) during the inference phase we want to leverage as much neighborhood information as possible!  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iee0U8KGURc8"
      },
      "source": [
        "# 3) Cluster Sampling\n",
        "\n",
        "As discussed in Module 7.2, we can also use subgraph (cluster) sampling to scale up GNN. Specifically, we will explore the methods proposed in Cluster-GCN ([Chiang et al. (2019)](https://arxiv.org/abs/1905.07953)), where we break our graph into smaller subgraphs to avoid the computational cost of training on the entire graph at once.\n",
        "\n",
        "In this final section, we will implement a vanilla Cluster-GCN and experiment with 3 different community partition algorithms.\n",
        "\n",
        "**NOTE:** the code in this section requires that you have run the `Setup`, `GNN Model` and `Training and Testing` cells of section 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BXjP79gUYir"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGQ_VKp8UOEm"
      },
      "source": [
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import community.community_louvain as community_louvain\n",
        "\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch.nn import Sequential, Linear, ReLU\n",
        "from deepsnap.dataset import GraphDataset\n",
        "from deepsnap.graph import Graph\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:   \n",
        "  pyg_dataset = Planetoid('./tmp', \"Cora\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzMatyCSUaB6"
      },
      "source": [
        "args = {\n",
        "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "    'dropout': 0.5,\n",
        "    'num_layers': 2,\n",
        "    'hidden_size': 64,\n",
        "    'lr': 0.005,\n",
        "    'epochs': 150,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekV-sokSUeLc"
      },
      "source": [
        "## Partitioning the Graph into Clusters\n",
        "\n",
        "We will experiment with three community detection / partition algorithms to partition our graph into different clusters:\n",
        "* [Kernighan–Lin algorithm (bisection)](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.kernighan_lin.kernighan_lin_bisection.html)\n",
        "* [Clauset-Newman-Moore greedy modularity maximization](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.modularity_max.greedy_modularity_communities.html#networkx.algorithms.community.modularity_max.greedy_modularity_communities)\n",
        "* [Louvain algorithm](https://python-louvain.readthedocs.io/en/latest/api.html)\n",
        "\n",
        "\n",
        "As a preprocessing step, we partition our graph into a list of seperate subgraphs using one of the three communitiy detection algorithms above. Then during training we iteratively train our vanilla Cluster-GNN model on a randomly selected subgraph, rather than on over the entire graph at once. To make training more stable, we discard any communities that have less than 10 nodes.\n",
        "\n",
        "Let's begin by defining the three partition algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8XeT005UcKh"
      },
      "source": [
        "def partition(G, method=\"louvain\"):\n",
        "    # TODO: Implement a function that takes a Networkx graph G and\n",
        "    # partitions the graph into communities using the specified graph\n",
        "    # partition algorithm.\n",
        "    # \n",
        "    # Return: A list of sets of nodes, one for each community!\n",
        "\n",
        "    communities = None\n",
        "\n",
        "    if method == \"louvain\":\n",
        "        ############# Your code here #############\n",
        "        ## (~9 line of code)\n",
        "        ## Note:\n",
        "        ## 1. Find a community mapping corresponding to the partition of the \n",
        "        ##    graph nodes which maximizes the modularity for the Louvain algorithm. \n",
        "        ##    Set your resolution to 10.\n",
        "        ## 2. Create a mapping of communities to a set of member nodes. \n",
        "        ## 3. Extract the node sets from each community and return \n",
        "        ##    as a list of sets. \n",
        "        ##    Hint: Perhaps a dictionary structure can assist.\n",
        "        ## 4. SET random_state = 8! \n",
        "\n",
        "        pass\n",
        "\n",
        "        ##########################################\n",
        "    elif method == \"bisection\":\n",
        "        ############# Your code here #############\n",
        "        ## (~1 line of code)\n",
        "        ## Note:\n",
        "        ## 1. The Kernigan-Lin algorithm ensures that nodes are partitioned into two \n",
        "        ##    primary communities.  \n",
        "        ## 2. Ensure that the resultant data structure is consistent with expected\n",
        "        ##    output.\n",
        "        ## 3. SET random_state = 8! \n",
        "\n",
        "        pass\n",
        "\n",
        "        ##########################################\n",
        "    elif method == \"greedy\":\n",
        "        ############# Your code here #############\n",
        "        ## (~1 line of code)\n",
        "        ## Note:\n",
        "        ## 1. Clauset-Newman-Moore greedy modularity maximization joins pair \n",
        "        ##    of communities nodes that most increases modularity until no such \n",
        "        ##    pair exists.\n",
        "\n",
        "        pass\n",
        "        \n",
        "        ##########################################\n",
        "\n",
        "    return communities \n",
        "\n",
        "\n",
        "def preprocess(G, node_label_index, method=\"louvain\"):\n",
        "    graphs = []\n",
        "    labeled_nodes = set(node_label_index.tolist())\n",
        "\n",
        "    communities = partition(G, method)\n",
        "\n",
        "    for community in communities:\n",
        "        nodes = set(community)\n",
        "        subgraph = G.subgraph(nodes)\n",
        "        # Make sure each subgraph has more than 10 nodes\n",
        "        if subgraph.number_of_nodes() > 10:\n",
        "            node_mapping = {node : i for i, node in enumerate(subgraph.nodes())}\n",
        "            subgraph = nx.relabel_nodes(subgraph, node_mapping)\n",
        "            # Get the id of the training set labeled node in the new graph\n",
        "            train_label_index = []\n",
        "            for node in labeled_nodes:\n",
        "                if node in node_mapping:\n",
        "                    # Append relabeled labeled node index\n",
        "                    train_label_index.append(node_mapping[node])\n",
        "\n",
        "            # Make sure the subgraph contains at least one training set labeled node\n",
        "            if len(train_label_index) > 0:\n",
        "                dg = Graph(subgraph)\n",
        "                # Update node_label_index\n",
        "                dg.node_label_index = torch.tensor(train_label_index, dtype=torch.long)\n",
        "                graphs.append(dg)\n",
        "    return graphs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKrUdkhC1-A3"
      },
      "source": [
        "## Experimenting with different graph partition algorithms\n",
        "\n",
        "We will now experiment with the three graph partition algorithms, using the resulting graph clusters to train our vanilla Cluster-GNN. We will first observe how each parition algorithm partitions the original graph. Then we will compare the perfomance of our vanilla Cluster-GNN trained using the different graph clustering techniques. Lastly, we will compare against training a vanilla GCN over the entire graph (refered to as Full-Batch training).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CYEamCAU-TJ"
      },
      "source": [
        "## **Question 3.1a:** How does the Louvain algorithm partition our graph? (1 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TrC6ybWU7eO"
      },
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  graphs_train, graphs_val, graphs_test = \\\n",
        "      GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "  graph_train = graphs_train[0]\n",
        "  graph_val = graphs_val[0]\n",
        "  graph_test = graphs_test[0]\n",
        "  graphs = preprocess(graph_train.G, graph_train.node_label_index, method=\"louvain\")\n",
        "  print()\n",
        "  print(\"Partitioning the graph in to {} communities\".format(len(graphs)))\n",
        "  avg_num_nodes = 0\n",
        "  avg_num_edges = 0\n",
        "  for graph in graphs:\n",
        "      avg_num_nodes += graph.num_nodes\n",
        "      avg_num_edges += graph.num_edges\n",
        "  avg_num_nodes = int(avg_num_nodes / len(graphs))\n",
        "  avg_num_edges = int(avg_num_edges / len(graphs))\n",
        "  print(\"Each community has {} nodes in average\".format(avg_num_nodes))\n",
        "  print(\"Each community has {} edges in average\".format(avg_num_edges))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O03uXIuGVIgJ"
      },
      "source": [
        "## **Question 3.1b:** Using Louvain clustering, what is the maximum test accuracy obtained by your vanilla Cluster-GCN? (4 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSbGf5ADVFQq"
      },
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  model = GNN(graph_train.num_node_features, args['hidden_size'], graph_train.num_node_labels, args).to(args['device'])\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "  louvain_best_model, louvain_accs = train(graphs, [graph_train, graph_val, graph_test], args, model, optimizer, mode=\"community\")\n",
        "  train_acc, val_acc, test_acc = test([graph_train, graph_val, graph_test], louvain_best_model, save_model_results=True, batch_type=\"cluster\", title=\"louvain\")\n",
        "  print('Best model:',\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * val_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CvTf0ANVO9U"
      },
      "source": [
        "## **Question 3.2a:** How does the Bisection algorithm partition our graph? (1 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkV0zlhgVJ7u"
      },
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  graphs_train, graphs_val, graphs_test = \\\n",
        "      GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "  graph_train = graphs_train[0]\n",
        "  graph_val = graphs_val[0]\n",
        "  graph_test = graphs_test[0]\n",
        "  graphs = preprocess(graph_train.G, graph_train.node_label_index, method=\"bisection\")\n",
        "  print(\"Partition the graph in to {} communities\".format(len(graphs)))\n",
        "  avg_num_nodes = 0\n",
        "  avg_num_edges = 0\n",
        "  for graph in graphs:\n",
        "      avg_num_nodes += graph.num_nodes\n",
        "      avg_num_edges += graph.num_edges\n",
        "  avg_num_nodes = int(avg_num_nodes / len(graphs))\n",
        "  avg_num_edges = int(avg_num_edges / len(graphs))\n",
        "  print(\"Each community has {} nodes in average\".format(avg_num_nodes))\n",
        "  print(\"Each community has {} edges in average\".format(avg_num_edges))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqMCvP8wVVms"
      },
      "source": [
        "## **Question 3.2b:** Using the Bisection algorithm to partition the graph, what is the maximum test accuracy obtained by your vanilla Cluster-GCN? (4 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1wgFg1bVRGY"
      },
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  model = GNN(graph_train.num_node_features, args['hidden_size'], graph_train.num_node_labels, args).to(args['device'])\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "  bisection_best_model, bisection_accs = train(graphs, [graph_train, graph_val, graph_test], args, model, optimizer, mode=\"community\")\n",
        "  train_acc, val_acc, test_acc = test([graph_train, graph_val, graph_test], bisection_best_model, save_model_results=True, batch_type=\"cluster\", title=\"bisection\")\n",
        "  print('Best model:',\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * val_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PROPwoOVcJy"
      },
      "source": [
        "## **Question 3.3a:** How does Greedy preprocessing partition our graph? (1 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3DVamWqVT92"
      },
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  graphs_train, graphs_val, graphs_test = \\\n",
        "      GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "  graph_train = graphs_train[0]\n",
        "  graph_val = graphs_val[0]\n",
        "  graph_test = graphs_test[0]\n",
        "  graphs = preprocess(graph_train.G, graph_train.node_label_index, method=\"greedy\")\n",
        "  print(\"Partition the graph in to {} communities\".format(len(graphs)))\n",
        "  avg_num_nodes = 0\n",
        "  avg_num_edges = 0\n",
        "  for graph in graphs:\n",
        "      avg_num_nodes += graph.num_nodes\n",
        "      avg_num_edges += graph.num_edges\n",
        "  avg_num_nodes = int(avg_num_nodes / len(graphs))\n",
        "  avg_num_edges = int(avg_num_edges / len(graphs))\n",
        "  print(\"Each community has {} nodes in average\".format(avg_num_nodes))\n",
        "  print(\"Each community has {} edges in average\".format(avg_num_edges))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93pR_-kxVgma"
      },
      "source": [
        "## **Question 3.3b:** Using Greedy preprocessing to partition the graph, what is the maximum test accuracy obtained by your vanilla Cluster-GCN? (4 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQgQY-jPVd_U"
      },
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ: \n",
        "  model = GNN(graph_train.num_node_features, args['hidden_size'], graph_train.num_node_labels, args).to(args['device'])\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "  greedy_best_model, greedy_accs = train(graphs, [graph_train, graph_val, graph_test], args, model, optimizer, mode=\"community\")\n",
        "  train_acc, val_acc, test_acc = test([graph_train, graph_val, graph_test], greedy_best_model, save_model_results=True, batch_type=\"cluster\", title=\"greedy\")\n",
        "  print('Best model:',\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * val_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5edKKT6Vk1C"
      },
      "source": [
        "## Full-Batch Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5tIXxC8ViFD"
      },
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  graphs_train, graphs_val, graphs_test = \\\n",
        "      GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True, fixed_split=True)\n",
        "\n",
        "  graph_train = graphs_train[0]\n",
        "  graph_val = graphs_val[0]\n",
        "  graph_test = graphs_test[0]\n",
        "\n",
        "  model = GNN(graph_train.num_node_features, args['hidden_size'], graph_train.num_node_labels, args).to(args['device'])\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "  graphs = [graph_train, graph_val, graph_test]\n",
        "  all_best_model, all_accs = train(graphs, graphs, args, model, optimizer, mode=\"all\")\n",
        "  train_acc, val_acc, test_acc = test([graph_train, graph_val, graph_test], all_best_model)\n",
        "  print('Best model:',\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * val_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RpuETv7Vpx0"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMK33kY5VmF5"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  louvain_results = np.array(louvain_accs)\n",
        "  bisection_results = np.array(bisection_accs)\n",
        "  greedy_results = np.array(greedy_accs)\n",
        "  all_results = np.array(all_accs)\n",
        "\n",
        "  x = np.arange(1, 151)\n",
        "\n",
        "  plt.figure(figsize=(9, 7))\n",
        "\n",
        "  plt.plot(x, louvain_results[:, 1], label=\"Louvain Validation\")\n",
        "  plt.plot(x, louvain_results[:, 2], label=\"Louvain Test\")\n",
        "  plt.plot(x, bisection_results[:, 1], label=\"Bisection Validation\")\n",
        "  plt.plot(x, bisection_results[:, 2], label=\"Bisection Test\")\n",
        "  plt.plot(x, greedy_results[:, 1], label=\"Greedy Validation\")\n",
        "  plt.plot(x, greedy_results[:, 2], label=\"Greedy Test\")\n",
        "  plt.plot(x, all_results[:, 1], label=\"All Validation\")\n",
        "  plt.plot(x, all_results[:, 2], label=\"All Test\")\n",
        "  plt.title('Model Accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfucBiYfVYFF"
      },
      "source": [
        "# Submission\n",
        "\n",
        "You will need to submit three files on Gradescope to complete this notebook. \n",
        "\n",
        "1.   Your completed *XCS224W_Colab5.ipynb*. From the \"File\" menu select \"Download .ipynb\" to save a local copy of your completed Colab. \n",
        "2.  *ogbn-arxiv_batch.csv* \n",
        "3.  *CORA_Node_batch_(0.7,0.9,1).csv*\n",
        "4.  *CORA_Node_batch_(0.3,0.5,1).csv*\n",
        "5.  *CORA_Node_cluster_louvain.csv*\n",
        "6.  *CORA_Node_cluster_greedy.csv*\n",
        "7.  *CORA_Node_cluster_bisection.csv*\n",
        "\n",
        "Download the csv files by selecting the *Folder* icon on the left panel. \n",
        "\n",
        "To submit your work, zip the files downloaded in steps 1-7 above and submit to gradescope. **NOTE:** DO NOT rename any of the downloaded files. "
      ]
    }
  ]
}